\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}

\title{Diffusion Models for Graph Signal Generation and Augmentation}
\author{Technical Report}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report describes the application of diffusion models to generate realistic graph signals on fixed meshes. We develop two approaches: (1) a GNN-based denoiser with edge features for direction-aware message passing, and (2) a Graph-Aware Diffusion (GAD) approach using heat equation dynamics on the graph Laplacian. A key insight is that the diffusion framework naturally supports data augmentation by controlling the noise injection level, allowing generation of signals that are ``similar but not identical'' to training examples.
\end{abstract}

\section{Introduction and Motivation}

\subsection{The Problem}
Given a set of graph signals defined on a fixed mesh (e.g., vertex positions from physics simulations), we want to generate new signals that:
\begin{enumerate}
    \item Look realistic (could plausibly come from the same physical process)
    \item Are diverse (not identical to training examples)
    \item Respect the underlying structure (mesh connectivity, boundary conditions)
\end{enumerate}

\subsection{Use Case: Data Augmentation}
The primary application is \textbf{data augmentation} for training graph neural networks. When real simulation data is expensive to generate, we want to:
\begin{itemize}
    \item Take a small set of real examples
    \item Generate many similar-but-different variations
    \item Use these to augment training data
\end{itemize}

\subsection{Training Data}
We use DeepMind's flag simulation dataset (MeshGraphNets):
\begin{itemize}
    \item 50 simulations $\times$ 401 timesteps = \textbf{20,050 training frames}
    \item Each frame: 1,579 vertices $\times$ 3 coordinates (xyz positions)
    \item Fixed mesh connectivity: 3,028 triangles
\end{itemize}

Each frame is treated as an independent graph signal $\mathbf{x} \in \mathbb{R}^{V \times 3}$.

\section{Methods We Explored (and Why They Failed)}

\subsection{Approach 1: Spectral Noise Filtering}
Filter white noise with the spectral envelope of real signals.

\textbf{Result}: Failed to capture boundary conditions (fixed pole moved freely), spatial coherence, and produced jittery outputs.

\subsection{Approach 2: Spectral VAE}
Variational autoencoder operating in the graph Fourier domain.

\textbf{Result}: On FEA data, outputs were nearly identical to inputs. Training data lacked diversity---generative models need varied examples to learn meaningful variation.

\section{Diffusion Models: The Solution}

\subsection{Why Diffusion?}
Diffusion models (Stable Diffusion, DALL-E 3) are state-of-the-art for generation because they:
\begin{itemize}
    \item Learn implicit constraints from data
    \item Generate high-quality, diverse outputs
    \item Support \textbf{controllable augmentation} via partial denoising
\end{itemize}

\subsection{The Diffusion Process}

\subsubsection{Forward Process (Adding Noise)}
Given a clean signal $\mathbf{x}_0$, progressively add Gaussian noise:
\begin{equation}
    \mathbf{x}_n = \sqrt{\bar{\alpha}_n} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_n} \, \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)
\end{equation}
where $\bar{\alpha}_n = \prod_{i=1}^{n} \alpha_i$ is the cumulative noise schedule, and $n$ is the \textbf{noise step}.

At $n = 0$: signal is clean. At $n = N$: signal is pure noise.

\subsubsection{Reverse Process (Denoising)}
A neural network $\boldsymbol{\epsilon}_\theta$ learns to predict the noise:
\begin{equation}
    \hat{\boldsymbol{\epsilon}} = \boldsymbol{\epsilon}_\theta(\mathbf{x}_n, n)
\end{equation}

\subsubsection{Training}
\begin{algorithm}[H]
\caption{Diffusion Training}
\begin{algorithmic}[1]
\Repeat
    \State Sample $\mathbf{x}_0 \sim$ training data (single frame)
    \State Sample $n \sim \text{Uniform}(1, N)$
    \State Sample $\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$
    \State $\mathbf{x}_n \gets \sqrt{\bar{\alpha}_n} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_n} \, \boldsymbol{\epsilon}$
    \State Gradient step on $\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_n, n)\|^2$
\Until{converged}
\end{algorithmic}
\end{algorithm}

\section{Key Insight: Augmentation via Partial Denoising}

Instead of starting from pure noise ($n = N$), start from a \textbf{real signal with added noise}:

\begin{algorithm}[H]
\caption{Signal Augmentation via Partial Denoising}
\begin{algorithmic}[1]
\State \textbf{Input:} Real signal $\mathbf{x}_0$, starting noise step $n^*$
\State Sample $\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$
\State $\mathbf{x}_{n^*} \gets \sqrt{\bar{\alpha}_{n^*}} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{n^*}} \, \boldsymbol{\epsilon}$
\For{$n = n^*, n^*-1, \ldots, 1, 0$}
    \State $\hat{\boldsymbol{\epsilon}} \gets \boldsymbol{\epsilon}_\theta(\mathbf{x}_n, n)$
    \State $\mathbf{x}_{n-1} \gets \text{DenoisingStep}(\mathbf{x}_n, \hat{\boldsymbol{\epsilon}}, n)$
\EndFor
\State \Return $\mathbf{x}_0$ \Comment{Augmented signal (similar to input)}
\end{algorithmic}
\end{algorithm}

\subsection{Controlling Similarity}
The starting noise step $n^*$ controls how different the output is from the input:

\begin{center}
\begin{tabular}{lll}
\toprule
$n^*$ & Noise Level & Result \\
\midrule
1000 & Maximum & Completely new signal \\
700 & High & Loosely similar \\
500 & Medium & Same structure, different details \\
300 & Low & Very close to original \\
0 & None & Identical to input \\
\bottomrule
\end{tabular}
\end{center}

\section{Two Denoiser Architectures}

We implement two approaches for comparison.

\subsection{Approach A: GNN with Edge Features (Ours)}

\subsubsection{Edge Features for Direction Awareness}
Standard GNN message passing treats all neighbors identically:
\begin{equation}
    h_i^{(l+1)} = h_i^{(l)} + \text{Aggregate}_{j \in \mathcal{N}(i)} \text{MLP}(h_i^{(l)}, h_j^{(l)})
\end{equation}

This is \textbf{isotropic}---unlike CNN kernels which have different weights for different positions.

We add \textbf{edge features} encoding the relative position of each neighbor:
\begin{equation}
    \mathbf{e}_{ij} = [\mathbf{p}_j - \mathbf{p}_i, \|\mathbf{p}_j - \mathbf{p}_i\|] \in \mathbb{R}^4
\end{equation}
where $\mathbf{p}_i, \mathbf{p}_j$ are vertex positions in the rest state.

The message function becomes:
\begin{equation}
    m_{j \to i} = \text{MLP}(h_i, h_j, \mathbf{e}_{ij})
\end{equation}

This allows the network to learn \textbf{direction-dependent} filters, similar to CNN kernels.

\subsubsection{Architecture}
\begin{center}
\begin{tabular}{ll}
\toprule
Component & Description \\
\midrule
Input projection & Linear: $3 \to D$ \\
Encoder & 4 MeshConv layers with residual connections \\
Timestep embedding & Sinusoidal + MLP \\
Decoder & 4 MeshConv layers with residual connections \\
Output projection & Linear: $D \to 3$ \\
\bottomrule
\end{tabular}
\end{center}

Parameters: $\sim$1M. Hidden dimension $D = 128$.

\subsection{Approach B: Graph-Aware Diffusion (GAD)}

Based on Rozada et al. (arXiv:2510.05036).

\subsubsection{Graph-Aware Forward Process}
Instead of isotropic Gaussian noise, GAD uses the \textbf{heat equation} on the graph:
\begin{equation}
    d\mathbf{x}_t = -c_t \mathbf{L}_\gamma \mathbf{x}_t \, dt + \sqrt{2 c_t} \sigma \, d\mathbf{w}_t
\end{equation}
where $\mathbf{L}$ is the graph Laplacian and $\mathbf{L}_\gamma = \mathbf{L} + \gamma \mathbf{I}$.

Key property: Noise spreads along graph edges. Smooth signals (low graph frequency) decay slower than high-frequency signals.

\subsubsection{Polynomial Graph Filter Denoiser}
The denoiser is a polynomial in the Laplacian:
\begin{equation}
    H(\mathbf{L}) = \sum_{k=0}^{K} \theta_k \mathbf{L}^k
\end{equation}

Each power $\mathbf{L}^k$ captures $k$-hop neighborhoods. The learnable coefficients $\theta_k$ weight contributions from different hop distances.

\subsection{Comparison}

\begin{center}
\begin{tabular}{lll}
\toprule
Aspect & GNN + Edge Features & GAD \\
\midrule
Forward process & Isotropic Gaussian & Heat equation on $\mathbf{L}$ \\
Denoiser & MLP message passing & Polynomial filter $H(\mathbf{L})$ \\
Edge features & Yes (direction vectors) & No (uses $\mathbf{L}^k$) \\
Parameters & $\sim$1M & $\sim$100K \\
Interpretability & Black-box MLP & Spectral (graph frequencies) \\
\bottomrule
\end{tabular}
\end{center}

\section{Implementation}

\subsection{Data Pipeline}
\begin{enumerate}
    \item Load flag simulation data (50 trajectories $\times$ 401 frames)
    \item Flatten to 20,050 individual frames
    \item Normalize (zero mean, unit variance)
    \item Split 90\% train, 10\% validation
\end{enumerate}

\subsection{Training Configuration}
\begin{center}
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
Batch size & 32 \\
Learning rate & $10^{-4}$ \\
Optimizer & AdamW \\
Noise steps & 1000 \\
Schedule & Cosine \\
Epochs & 100 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Usage}
\begin{verbatim}
python setup_flag_data.py      # Download and prepare data
python train_flag_diffusion.py  # Train model
\end{verbatim}

\section{Summary}

\begin{enumerate}
    \item \textbf{Simple approaches failed}: Spectral filtering and VAEs couldn't capture the full structure of graph signals.

    \item \textbf{Diffusion learns holistically}: By learning to denoise at all noise levels, the model captures spatial coherence and physical constraints implicitly.

    \item \textbf{Augmentation is built-in}: Partial denoising provides controllable augmentation---start from intermediate noise level to generate similar-but-different signals.

    \item \textbf{Two architectures}:
    \begin{itemize}
        \item GNN with edge features: direction-aware, like CNN kernels
        \item GAD: graph-aware noise via heat equation, polynomial filter denoiser
    \end{itemize}
\end{enumerate}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{ddpm}
Ho, J., Jain, A., \& Abbeel, P. (2020).
Denoising Diffusion Probabilistic Models.
\textit{NeurIPS}.

\bibitem{gad}
Rozada, S., et al. (2025).
Graph-Aware Diffusion for Signal Generation.
\textit{arXiv:2510.05036}.

\bibitem{meshgraphnets}
Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., \& Battaglia, P. W. (2021).
Learning Mesh-Based Simulation with Graph Networks.
\textit{ICLR}.

\bibitem{shuman}
Shuman, D. I., et al. (2013).
The Emerging Field of Signal Processing on Graphs.
\textit{IEEE Signal Processing Magazine}.

\bibitem{sdedit}
Meng, C., et al. (2022).
SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations.
\textit{ICLR}.

\end{thebibliography}

\end{document}
