\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}

\title{Diffusion Models for Graph Signal Generation and Augmentation}
\author{Technical Report}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report describes the application of diffusion models to generate realistic graph signals on fixed meshes. We develop two approaches: (1) a GNN-based denoiser with edge features for direction-aware message passing, and (2) a Graph-Aware Diffusion (GAD) approach using heat equation dynamics on the graph Laplacian. A key insight is that the diffusion framework naturally supports data augmentation by controlling the noise injection level, allowing generation of signals that are ``similar but not identical'' to training examples.
\end{abstract}

\section{Introduction and Motivation}

\subsection{The Problem}
Given a set of graph signals defined on a fixed mesh (e.g., vertex positions from physics simulations), we want to generate new signals that:
\begin{enumerate}
    \item Look realistic (could plausibly come from the same physical process)
    \item Are diverse (not identical to training examples)
    \item Respect the underlying structure (mesh connectivity, boundary conditions)
\end{enumerate}

\subsection{Use Case: Data Augmentation}
The primary application is \textbf{data augmentation} for training graph neural networks. When real simulation data is expensive to generate, we want to:
\begin{itemize}
    \item Take a small set of real examples
    \item Generate many similar-but-different variations
    \item Use these to augment training data
\end{itemize}

\subsection{Training Data}
We use DeepMind's flag simulation dataset (MeshGraphNets):
\begin{itemize}
    \item 50 simulations $\times$ 401 timesteps = \textbf{20,050 training frames}
    \item Each frame: 1,579 vertices $\times$ 3 coordinates (xyz positions)
    \item Fixed mesh connectivity: 3,028 triangles
\end{itemize}

Each frame is treated as an independent graph signal $\mathbf{x} \in \mathbb{R}^{V \times 3}$.

\subsubsection{Custom Simulation (Optional)}
We also provide a GPU-accelerated cloth simulation (\texttt{simulate\_flag.py}) for generating additional training data:
\begin{itemize}
    \item Position-based dynamics with vectorized constraint solving
    \item Chaotic wind forces with multi-frequency sinusoidal gusts
    \item Configurable temporal stride (default: 100 steps) ensures statistically independent frames
    \item Avoids the problem of highly correlated consecutive frames
\end{itemize}

\begin{verbatim}
python simulate_flag.py --record --stride 100 --frames 100000
\end{verbatim}

\section{Methods We Explored (and Why They Failed)}

\subsection{Approach 1: Spectral Noise Filtering}
Filter white noise with the spectral envelope of real signals.

\textbf{Result}: Failed to capture boundary conditions (fixed pole moved freely), spatial coherence, and produced jittery outputs.

\subsection{Approach 2: Spectral VAE}
Variational autoencoder operating in the graph Fourier domain.

\textbf{Result}: On FEA data, outputs were nearly identical to inputs. Training data lacked diversity---generative models need varied examples to learn meaningful variation.

\section{Diffusion Models: The Solution}

\subsection{Why Diffusion?}
Diffusion models (Stable Diffusion, DALL-E 3) are state-of-the-art for generation because they:
\begin{itemize}
    \item Learn implicit constraints from data
    \item Generate high-quality, diverse outputs
    \item Support \textbf{controllable augmentation} via partial denoising
\end{itemize}

\subsection{The Diffusion Process}

\subsubsection{Forward Process (Adding Noise)}
Given a clean signal $\mathbf{x}_0$, progressively add Gaussian noise:
\begin{equation}
    \mathbf{x}_n = \sqrt{\bar{\alpha}_n} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_n} \, \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)
\end{equation}
where $\bar{\alpha}_n = \prod_{i=1}^{n} \alpha_i$ is the cumulative noise schedule, and $n$ is the \textbf{noise step}.

At $n = 0$: signal is clean. At $n = N$: signal is pure noise.

\subsubsection{Reverse Process (Denoising)}
A neural network $\boldsymbol{\epsilon}_\theta$ learns to predict the noise:
\begin{equation}
    \hat{\boldsymbol{\epsilon}} = \boldsymbol{\epsilon}_\theta(\mathbf{x}_n, n)
\end{equation}

\subsubsection{Training}
\begin{algorithm}[H]
\caption{Diffusion Training}
\begin{algorithmic}[1]
\Repeat
    \State Sample $\mathbf{x}_0 \sim$ training data (single frame)
    \State Sample $n \sim \text{Uniform}(1, N)$
    \State Sample $\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$
    \State $\mathbf{x}_n \gets \sqrt{\bar{\alpha}_n} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_n} \, \boldsymbol{\epsilon}$
    \State Gradient step on $\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_n, n)\|^2$
\Until{converged}
\end{algorithmic}
\end{algorithm}

\section{Key Insight: Augmentation via Partial Denoising}

Instead of starting from pure noise ($n = N$), start from a \textbf{real signal with added noise}:

\begin{algorithm}[H]
\caption{Signal Augmentation via Partial Denoising}
\begin{algorithmic}[1]
\State \textbf{Input:} Real signal $\mathbf{x}_0$, starting noise step $n^*$
\State Sample $\boldsymbol{\epsilon} \sim \mathcal{N}(0, I)$
\State $\mathbf{x}_{n^*} \gets \sqrt{\bar{\alpha}_{n^*}} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{n^*}} \, \boldsymbol{\epsilon}$
\For{$n = n^*, n^*-1, \ldots, 1, 0$}
    \State $\hat{\boldsymbol{\epsilon}} \gets \boldsymbol{\epsilon}_\theta(\mathbf{x}_n, n)$
    \State $\mathbf{x}_{n-1} \gets \text{DenoisingStep}(\mathbf{x}_n, \hat{\boldsymbol{\epsilon}}, n)$
\EndFor
\State \Return $\mathbf{x}_0$ \Comment{Augmented signal (similar to input)}
\end{algorithmic}
\end{algorithm}

\subsection{Controlling Similarity}
The starting noise step $n^*$ controls how different the output is from the input:

\begin{center}
\begin{tabular}{lll}
\toprule
$n^*$ & Noise Level & Result \\
\midrule
1000 & Maximum & Completely new signal \\
700 & High & Loosely similar \\
500 & Medium & Same structure, different details \\
300 & Low & Very close to original \\
0 & None & Identical to input \\
\bottomrule
\end{tabular}
\end{center}

\section{Two Denoiser Architectures}

We implement two approaches for comparison.

\subsection{Approach A: GNN with Edge Features (Ours)}

\subsubsection{Edge Features for Direction Awareness}
Standard GNN message passing treats all neighbors identically:
\begin{equation}
    h_i^{(l+1)} = h_i^{(l)} + \text{Aggregate}_{j \in \mathcal{N}(i)} \text{MLP}(h_i^{(l)}, h_j^{(l)})
\end{equation}

This is \textbf{isotropic}---unlike CNN kernels which have different weights for different positions.

We add \textbf{edge features} encoding the relative position of each neighbor:
\begin{equation}
    \mathbf{e}_{ij} = [\mathbf{p}_j - \mathbf{p}_i, \|\mathbf{p}_j - \mathbf{p}_i\|] \in \mathbb{R}^4
\end{equation}
where $\mathbf{p}_i, \mathbf{p}_j$ are vertex positions in the rest state.

The message function becomes:
\begin{equation}
    m_{j \to i} = \text{MLP}(h_i, h_j, \mathbf{e}_{ij})
\end{equation}

This allows the network to learn \textbf{direction-dependent} filters, similar to CNN kernels.

\subsubsection{Architecture}
\begin{center}
\begin{tabular}{ll}
\toprule
Component & Description \\
\midrule
Input projection & Linear: $3 \to D$ \\
Encoder & 4 MeshConv layers with residual connections \\
Timestep embedding & Sinusoidal + MLP \\
Decoder & 4 MeshConv layers with residual connections \\
Output projection & Linear: $D \to 3$ \\
\bottomrule
\end{tabular}
\end{center}

Parameters: $\sim$534K. Hidden dimension $D = 128$.

\subsection{Approach B: Graph-Aware Diffusion (GAD)}

Based on Rozada et al. (arXiv:2510.05036).

\subsubsection{Graph-Aware Forward Process}
Instead of isotropic Gaussian noise, GAD uses the \textbf{heat equation} on the graph:
\begin{equation}
    d\mathbf{x}_t = -c_t \mathbf{L}_\gamma \mathbf{x}_t \, dt + \sqrt{2 c_t} \sigma \, d\mathbf{w}_t
\end{equation}
where $\mathbf{L}$ is the graph Laplacian and $\mathbf{L}_\gamma = \mathbf{L} + \gamma \mathbf{I}$.

Key property: Noise spreads along graph edges. Smooth signals (low graph frequency) decay slower than high-frequency signals.

\subsubsection{Polynomial Graph Filter Denoiser}
The denoiser is a polynomial in the Laplacian:
\begin{equation}
    H(\mathbf{L}) = \sum_{k=0}^{K} \theta_k \mathbf{L}^k
\end{equation}

Each power $\mathbf{L}^k$ captures $k$-hop neighborhoods. The learnable coefficients $\theta_k$ weight contributions from different hop distances.

\subsection{Comparison}

\begin{center}
\begin{tabular}{lll}
\toprule
Aspect & GNN + Edge Features & GAD \\
\midrule
Forward process & Isotropic Gaussian & Heat equation on $\mathbf{L}$ \\
Denoiser & MLP message passing & Polynomial filter $H(\mathbf{L})$ \\
Edge features & Yes (direction vectors) & No (uses $\mathbf{L}^k$) \\
Parameters & $\sim$534K & $\sim$100K \\
Interpretability & Black-box MLP & Spectral (graph frequencies) \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Spectral Analysis: High-Pass vs Low-Pass}

A key difference between the two approaches lies in how they handle graph frequencies.

\subsubsection{The Graph Laplacian as a High-Pass Filter}
The Laplacian measures how different each vertex is from its neighbors:
\begin{equation}
    (\mathbf{L}\mathbf{x})_i = \sum_{j \in \mathcal{N}(i)} (x_i - x_j)
\end{equation}

The eigenvalues $\lambda$ of $\mathbf{L}$ correspond to graph frequencies:
\begin{itemize}
    \item $\lambda = 0$: constant signal (DC component)
    \item $\lambda$ small: smooth, slowly-varying signals (low frequency)
    \item $\lambda$ large: rapidly-varying signals (high frequency)
\end{itemize}

\subsubsection{GAD's Forward Process Destroys High Frequencies First}
The heat kernel solution is:
\begin{equation}
    \mathbf{x}(t) = \exp(-t\mathbf{L}) \mathbf{x}(0)
\end{equation}

In the spectral domain, each eigenvalue $\lambda$ is attenuated by $\exp(-t\lambda)$:
\begin{itemize}
    \item Small $\lambda$ (low freq): $\exp(-t\lambda) \approx 1$ --- \textbf{preserved}
    \item Large $\lambda$ (high freq): $\exp(-t\lambda) \approx 0$ --- \textbf{destroyed}
\end{itemize}

This is inherently a \textbf{low-pass} forward process. By the time $t$ is large (high noise), the high-frequency components are completely gone. The denoiser cannot recover information that has been erased.

\subsubsection{Our Approach Preserves All Frequencies}
With isotropic Gaussian noise:
\begin{equation}
    \mathbf{x}_n = \sqrt{\bar{\alpha}_n} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_n} \, \boldsymbol{\epsilon}
\end{equation}

All frequencies are corrupted equally---both low and high frequency components are scaled by $\sqrt{\bar{\alpha}_n}$ and buried under noise. But they are still \textbf{present} in the noisy signal, not destroyed. The denoiser can learn to recover both.

\subsubsection{Implications}

\begin{center}
\begin{tabular}{lll}
\toprule
 & GAD (Heat Equation) & Ours (Isotropic Noise) \\
\midrule
Forward process & Low-pass & All-pass \\
High freq at $n = N$ & Gone (irrecoverable) & Buried but present \\
Denoiser task & Recover low freq, hallucinate high freq & Recover all freq \\
Best for & Smooth signals & Signals with fine detail \\
\bottomrule
\end{tabular}
\end{center}

For mesh data with sharp features (edges, creases, boundaries), our isotropic approach may be preferable since it doesn't preferentially destroy high-frequency geometric detail.

\section{Implementation}

\subsection{Data Pipeline}
\begin{enumerate}
    \item Load flag simulation data (50 trajectories $\times$ 401 frames)
    \item Flatten to 20,050 individual frames
    \item Normalize to $[-1, 1]$ range via min-max scaling (standard for DDPM)
    \item Split 90\% train, 10\% validation
\end{enumerate}

\subsection{Training Configuration}
\begin{center}
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
Batch size & 32 \\
Learning rate & $10^{-4}$ \\
Optimizer & AdamW \\
Noise steps & 1000 \\
Schedule & Cosine \\
Epochs & 100 \\
Mixed precision & AMP (on CUDA) \\
Gradient clipping & 1.0 \\
\bottomrule
\end{tabular}
\end{center}

Mixed precision training via PyTorch AMP (Automatic Mixed Precision) is enabled on CUDA devices for faster training with reduced memory usage.

\subsection{Usage}
\begin{verbatim}
python setup_flag_data.py                          # Download and prepare data
python simulate_flag.py --record --stride 100      # (Optional) Generate more data
python train_flag_diffusion.py                     # Train model
\end{verbatim}

\section{Relationship to Prior Work}

This work combines several established methods. We explicitly document the provenance of each component to clarify that no novel algorithms are introduced---our contribution is the application of these methods to graph signal augmentation.

\subsection{Component Attribution}

\begin{center}
\begin{tabular}{p{4cm}p{4cm}p{5cm}}
\toprule
\textbf{Our Component} & \textbf{Established Method} & \textbf{Reference} \\
\midrule
Forward/reverse diffusion process & DDPM & Ho et al.~\cite{ddpm} \\
Cosine noise schedule & Improved DDPM & Nichol \& Dhariwal~\cite{iddpm} \\
Partial denoising for augmentation & SDEdit & Meng et al.~\cite{sdedit} \\
Message passing with edge features & MeshGraphNets / MPNN & Pfaff et al.~\cite{meshgraphnets}, Gilmer et al.~\cite{mpnn} \\
Sinusoidal timestep embedding & Transformer / DDPM & Vaswani et al., Ho et al.~\cite{ddpm} \\
Graph Laplacian analysis & Graph Signal Processing & Shuman et al.~\cite{shuman} \\
Heat equation forward process & GAD & Rozada et al.~\cite{gad} \\
Polynomial graph filters & GAD / Spectral GNNs & Rozada et al.~\cite{gad} \\
\bottomrule
\end{tabular}
\end{center}

\subsection{How We Use Each Method}

\subsubsection{DDPM (Ho et al., 2020)}
We use the standard DDPM formulation unchanged:
\begin{itemize}
    \item Forward process: $\mathbf{x}_n = \sqrt{\bar{\alpha}_n} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_n} \boldsymbol{\epsilon}$
    \item Training objective: $\mathcal{L} = \mathbb{E}[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_n, n)\|^2]$
    \item Reverse sampling: iterative denoising from $n=N$ to $n=0$
\end{itemize}
The only difference is that our signal $\mathbf{x} \in \mathbb{R}^{V \times 3}$ is a graph signal rather than an image.

\subsubsection{Improved DDPM / Cosine Schedule (Nichol \& Dhariwal, 2021)}
We use the cosine schedule exactly as proposed:
\begin{equation}
    \bar{\alpha}_n = \frac{f(n)}{f(0)}, \quad f(t) = \cos\left(\frac{t/N + s}{1+s} \cdot \frac{\pi}{2}\right)^2
\end{equation}
with $s = 0.008$. This provides smoother noise progression than linear schedules.

\subsubsection{SDEdit (Meng et al., 2022)}
Our augmentation procedure is SDEdit applied to graph signals:
\begin{enumerate}
    \item Start with real signal $\mathbf{x}_0$
    \item Add noise to intermediate step $n^*$: $\mathbf{x}_{n^*} = \sqrt{\bar{\alpha}_{n^*}} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_{n^*}} \boldsymbol{\epsilon}$
    \item Denoise from $n^*$ back to 0
\end{enumerate}
This is identical to SDEdit's ``img2img'' procedure, substituting graph signals for images.

\subsubsection{MeshGraphNets / MPNN (Pfaff et al., 2021; Gilmer et al., 2017)}
Our GNN denoiser uses the standard message passing framework:
\begin{equation}
    m_{j \to i} = \phi_e(h_i, h_j, \mathbf{e}_{ij}), \quad h_i' = \phi_v\left(h_i, \sum_{j \in \mathcal{N}(i)} m_{j \to i}\right)
\end{equation}
where $\phi_e$ and $\phi_v$ are MLPs. Edge features $\mathbf{e}_{ij} = [\mathbf{p}_j - \mathbf{p}_i, \|\mathbf{p}_j - \mathbf{p}_i\|]$ encode relative positions, exactly as in MeshGraphNets.

\subsubsection{GAD (Rozada et al., 2025)}
For comparison, we implement GAD's approach:
\begin{itemize}
    \item Heat equation forward process on the graph Laplacian
    \item Polynomial graph filter denoiser: $H(\mathbf{L}) = \sum_k \theta_k \mathbf{L}^k$
\end{itemize}

\subsection{What Is Not Novel}

To be explicit, we do \textbf{not} claim novelty for:
\begin{itemize}
    \item The diffusion framework (DDPM)
    \item The noise schedule (Improved DDPM)
    \item The partial denoising augmentation strategy (SDEdit)
    \item The message passing architecture (MPNN/MeshGraphNets)
    \item The graph signal processing perspective (Shuman et al.)
\end{itemize}

\subsection{Our Contribution}

Our contribution is \textbf{applying} these established methods to:
\begin{enumerate}
    \item Generate realistic graph signals on fixed meshes
    \item Provide controllable data augmentation for training physics-based GNNs
    \item Compare isotropic (DDPM) vs graph-aware (GAD) forward processes
\end{enumerate}

This is an \textbf{application paper}, demonstrating that standard diffusion techniques transfer effectively to the graph signal domain.

\section{Summary}

\begin{enumerate}
    \item \textbf{Simple approaches failed}: Spectral filtering and VAEs couldn't capture the full structure of graph signals.

    \item \textbf{Diffusion learns holistically}: By learning to denoise at all noise levels, the model captures spatial coherence and physical constraints implicitly.

    \item \textbf{Augmentation is built-in}: Partial denoising provides controllable augmentation---start from intermediate noise level to generate similar-but-different signals.

    \item \textbf{Two architectures}:
    \begin{itemize}
        \item GNN with edge features: direction-aware, like CNN kernels
        \item GAD: graph-aware noise via heat equation, polynomial filter denoiser
    \end{itemize}
\end{enumerate}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{ddpm}
Ho, J., Jain, A., \& Abbeel, P. (2020).
Denoising Diffusion Probabilistic Models.
\textit{NeurIPS}.

\bibitem{iddpm}
Nichol, A. \& Dhariwal, P. (2021).
Improved Denoising Diffusion Probabilistic Models.
\textit{ICML}.

\bibitem{mpnn}
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., \& Dahl, G. E. (2017).
Neural Message Passing for Quantum Chemistry.
\textit{ICML}.

\bibitem{gad}
Rozada, S., et al. (2025).
Graph-Aware Diffusion for Signal Generation.
\textit{arXiv:2510.05036}.

\bibitem{meshgraphnets}
Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., \& Battaglia, P. W. (2021).
Learning Mesh-Based Simulation with Graph Networks.
\textit{ICLR}.

\bibitem{shuman}
Shuman, D. I., et al. (2013).
The Emerging Field of Signal Processing on Graphs.
\textit{IEEE Signal Processing Magazine}.

\bibitem{sdedit}
Meng, C., et al. (2022).
SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations.
\textit{ICLR}.

\end{thebibliography}

\end{document}
